#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æ¤œç´¢ãƒ»é€£çµ¡å…ˆåé›†ãƒ„ãƒ¼ãƒ«

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
pip install streamlit requests beautifulsoup4 google-api-python-client gspread google-auth pandas
"""

import os
import re
import json
import time
import random
import tempfile
import datetime
import streamlit as st
import pandas as pd
import requests
from typing import List, Dict, Any, Optional, Union, Tuple
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
import gspread
from google.oauth2.service_account import Credentials
import base64
import io

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æ¤œç´¢ãƒ»é€£çµ¡å…ˆåé›†ãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'search_results' not in st.session_state:
    st.session_state.search_results = []
if 'filtered_results' not in st.session_state:
    st.session_state.filtered_results = []
if 'api_keys' not in st.session_state:
    st.session_state.api_keys = {}
if 'csv_filename' not in st.session_state:
    st.session_state.csv_filename = "influencer_contacts.csv"


class InfluencerFinder:
    """ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æƒ…å ±åé›†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºç”¨ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
        
        # SNSãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã®URLæŠ½å‡ºç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.sns_patterns = {
            'instagram': [
                re.compile(r'https?://(?:www\.)?instagram\.com/[a-zA-Z0-9_.]+/?'),
                re.compile(r'instagram\.com/[a-zA-Z0-9_.]+/?')
            ],
            'tiktok': [
                re.compile(r'https?://(?:www\.)?tiktok\.com/@[a-zA-Z0-9_.]+/?'),
                re.compile(r'tiktok\.com/@[a-zA-Z0-9_.]+/?')
            ],
            'youtube': [
                re.compile(r'https?://(?:www\.)?youtube\.com/(?:channel|user|c)/[a-zA-Z0-9_-]+/?'),
                re.compile(r'https?://(?:www\.)?youtube\.com/@[a-zA-Z0-9_-]+/?'),
                re.compile(r'youtube\.com/(?:channel|user|c)/[a-zA-Z0-9_-]+/?'),
                re.compile(r'youtube\.com/@[a-zA-Z0-9_-]+/?')
            ],
            'x_twitter': [
                re.compile(r'https?://(?:www\.)?(?:twitter|x)\.com/[a-zA-Z0-9_]+/?'),
                re.compile(r'(?:twitter|x)\.com/[a-zA-Z0-9_]+/?')
            ],
            'facebook': [
                re.compile(r'https?://(?:www\.)?facebook\.com/[a-zA-Z0-9.]+/?'),
                re.compile(r'facebook\.com/[a-zA-Z0-9.]+/?')
            ]
        }
    
    def search_google(self, query: str, api_key: str, cx: str, num_results: int = 20) -> List[Dict[str, Any]]:
        """
        Google Custom Search APIã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            api_key: Google API Key
            cx: Custom Search Engine ID
            num_results: å–å¾—ã™ã‚‹çµæœã®æ•°
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        try:
            service = build("customsearch", "v1", developerKey=api_key)
            
            # è¤‡æ•°ãƒšãƒ¼ã‚¸ã®çµæœã‚’å–å¾—
            all_results = []
            pages = (num_results + 9) // 10  # 10ä»¶ãšã¤å–å¾—ã™ã‚‹ãŸã‚ã€å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
            
            for page in range(pages):
                start_index = page * 10 + 1
                
                # APIå‘¼ã³å‡ºã—
                result = service.cse().list(
                    q=query,
                    cx=cx,
                    start=start_index
                ).execute()
                
                # çµæœãŒãªã„å ´åˆã¯çµ‚äº†
                if 'items' not in result:
                    break
                    
                all_results.extend(result['items'])
                
                # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é…æ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
                time.sleep(0.5)
            
            # æ¤œç´¢çµæœã‚’æ•´å½¢
            formatted_results = []
            for item in all_results[:num_results]:  # æŒ‡å®šã•ã‚ŒãŸæ•°ã ã‘å–å¾—
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¼æ¥­å/å€‹äººåã‚’æ¨å®š
                title = item.get('title', '')
                company_name = self._extract_company_name(title)
                
                result = {
                    'company_name': company_name,
                    'website_url': item.get('link', ''),
                    'title': title,
                    'snippet': item.get('snippet', ''),
                    'instagram_url': '',
                    'tiktok_url': '',
                    'youtube_url': '',
                    'x_url': '',
                    'facebook_url': '',
                    'email': '',
                    'source': 'Google Search'
                }
                formatted_results.append(result)
            
            return formatted_results
                
        except Exception as e:
            st.error(f"Googleæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return []
    
    def _extract_company_name(self, title: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¼æ¥­å/å€‹äººåã‚’æ¨å®šã™ã‚‹
        
        Args:
            title: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
            
        Returns:
            æ¨å®šã•ã‚ŒãŸä¼æ¥­å/å€‹äººå
        """
        # ã€Œ|ã€ã€Œ-ã€ã€Œ:ã€ãªã©ã§åˆ†å‰²ã—ã€æœ€åˆã®éƒ¨åˆ†ã‚’å–å¾—
        separators = ['|', '-', ':', 'ï¼š', 'ï¼', '/', 'ï½œ']
        company_name = title
        
        for sep in separators:
            if sep in company_name:
                parts = company_name.split(sep)
                company_name = parts[0].strip()
        
        return company_name
    
    def extract_contact_info(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        URLã‹ã‚‰ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            result: æ¤œç´¢çµæœã®è¾æ›¸
            
        Returns:
            æ›´æ–°ã•ã‚ŒãŸæ¤œç´¢çµæœè¾æ›¸
        """
        url = result.get('website_url', '')
        if not url:
            return result
        
        try:
            # Webãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # ãƒšãƒ¼ã‚¸ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèª
            if 'Content-Type' in response.headers:
                content_type = response.headers['Content-Type']
                if 'charset=' in content_type:
                    encoding = content_type.split('charset=')[1].split(';')[0].strip()
                    response.encoding = encoding
            
            # BeautifulSoupã§HTMLã‚’è§£æ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’å‰Šé™¤
            for script in soup(["script", "style"]):
                script.extract()
            
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = soup.get_text()
            
            # HTMLã‚½ãƒ¼ã‚¹å…¨ä½“
            html_source = response.text
            
            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º
            emails = self._extract_emails(text)
            emails.extend(self._extract_emails(html_source))  # HTMLã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚‚æŠ½å‡º
            if emails:
                result['email'] = emails[0]  # æœ€åˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ä½¿ç”¨
            
            # SNSãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            sns_urls = self._extract_sns_urls(html_source)
            sns_urls.update(self._extract_sns_urls_from_links(soup))
            
            # çµæœã‚’æ›´æ–°
            for sns_type, url in sns_urls.items():
                if url:
                    result[f'{sns_type}_url'] = url
            
            return result
            
        except Exception as e:
            st.warning(f"URLè§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {url} - {str(e)}")
            return result
    
    def _extract_emails(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º
        
        Args:
            text: æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        if not text:
            return []
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ¤œç´¢
        emails = self.email_pattern.findall(text)
        
        # é‡è¤‡ã‚’å‰Šé™¤ã—ã¦è¿”ã™
        return list(set(emails))
    
    def _extract_sns_urls(self, text: str) -> Dict[str, str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰SNS URLã‚’æŠ½å‡º
        
        Args:
            text: æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            SNSã‚¿ã‚¤ãƒ—ã¨URLã®è¾æ›¸
        """
        sns_urls = {
            'instagram': '',
            'tiktok': '',
            'youtube': '',
            'x': '',
            'facebook': ''
        }
        
        # å„SNSã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§URLã‚’æ¤œç´¢
        for sns_type, patterns in self.sns_patterns.items():
            for pattern in patterns:
                matches = pattern.findall(text)
                if matches:
                    # æœ€åˆã®ãƒãƒƒãƒã‚’ä½¿ç”¨
                    url = matches[0]
                    
                    # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãŒãªã„å ´åˆã¯è¿½åŠ 
                    if not url.startswith('http'):
                        url = 'https://' + url
                    
                    # SNSã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚­ãƒ¼ã‚’è¨­å®š
                    if sns_type == 'x_twitter':
                        sns_urls['x'] = url
                    else:
                        sns_urls[sns_type] = url
                    break
        
        return sns_urls
    
    def _extract_sns_urls_from_links(self, soup: BeautifulSoup) -> Dict[str, str]:
        """
        HTMLã®ãƒªãƒ³ã‚¯ã‹ã‚‰SNS URLã‚’æŠ½å‡º
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            SNSã‚¿ã‚¤ãƒ—ã¨URLã®è¾æ›¸
        """
        sns_urls = {
            'instagram': '',
            'tiktok': '',
            'youtube': '',
            'x': '',
            'facebook': ''
        }
        
        # aã‚¿ã‚°ã‚’å–å¾—
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link.get('href', '')
            
            # Instagramãƒªãƒ³ã‚¯
            if 'instagram.com' in href:
                sns_urls['instagram'] = href
            
            # TikTokãƒªãƒ³ã‚¯
            elif 'tiktok.com' in href:
                sns_urls['tiktok'] = href
            
            # YouTubeãƒªãƒ³ã‚¯
            elif 'youtube.com' in href or 'youtu.be' in href:
                sns_urls['youtube'] = href
            
            # Twitter/Xãƒªãƒ³ã‚¯
            elif 'twitter.com' in href or 'x.com' in href:
                sns_urls['x'] = href
            
            # Facebookãƒªãƒ³ã‚¯
            elif 'facebook.com' in href:
                sns_urls['facebook'] = href
        
        return sns_urls
    
    def process_search_results(self, results: List[Dict[str, Any]], max_pages_to_scan: int = 20) -> List[Dict[str, Any]]:
        """
        æ¤œç´¢çµæœã‚’å‡¦ç†ã—ã€å„URLã‹ã‚‰ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
            max_pages_to_scan: ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ãƒšãƒ¼ã‚¸ã®æœ€å¤§æ•°
            
        Returns:
            ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ä»˜ãã®æ¤œç´¢çµæœ
        """
        processed_results = []
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, result in enumerate(results[:max_pages_to_scan]):
            status_text.text(f"å‡¦ç†ä¸­... {i+1}/{min(len(results), max_pages_to_scan)}: {result['title']}")
            progress_percent = (i + 1) / min(len(results), max_pages_to_scan)
            progress_bar.progress(progress_percent)
            
            # ãƒªãƒ³ã‚¯å…ˆã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã‚’æŠ½å‡º
            if result.get('website_url'):
                updated_result = self.extract_contact_info(result.copy())
                processed_results.append(updated_result)
            else:
                processed_results.append(result)
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(random.uniform(0.5, 1.5))
        
        status_text.text("å‡¦ç†å®Œäº†!")
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()
        
        return processed_results


def export_to_csv(data: List[Dict[str, Any]], filename: str) -> Optional[str]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    
    Args:
        data: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        filename: å‡ºåŠ›ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«å
        
    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ç”¨ã®HTML
    """
    if not data:
        return None
    
    try:
        # å‡ºåŠ›ã‚«ãƒ©ãƒ ã‚’å®šç¾©
        columns = [
            'company_name', 'website_url', 'instagram_url', 'tiktok_url', 
            'youtube_url', 'x_url', 'facebook_url', 'email'
        ]
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’æŠ½å‡º
        filtered_data = []
        for item in data:
            filtered_item = {col: item.get(col, '') for col in columns}
            filtered_data.append(filtered_item)
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(filtered_data)
        
        # CSVãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        csv = df.to_csv(index=False)
        
        # CSVã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        b64 = base64.b64encode(csv.encode()).decode()
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã®ä½œæˆ
        href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
        return href
        
    except Exception as e:
        st.error(f"CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None


def export_to_google_spreadsheet(data: List[Dict[str, Any]], service_account_json: str, spreadsheet_id: str, sheet_name: str) -> bool:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    
    Args:
        data: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        service_account_json: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã®å†…å®¹
        spreadsheet_id: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID
        sheet_name: ã‚·ãƒ¼ãƒˆå
        
    Returns:
        æˆåŠŸã—ãŸã‹ã©ã†ã‹
    """
    try:
        # å‡ºåŠ›ã‚«ãƒ©ãƒ ã‚’å®šç¾©
        columns = [
            'company_name', 'website_url', 'instagram_url', 'tiktok_url', 
            'youtube_url', 'x_url', 'facebook_url', 'email'
        ]
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’æŠ½å‡º
        filtered_data = []
        for item in data:
            filtered_item = {col: item.get(col, '') for col in columns}
            filtered_data.append(filtered_item)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as temp:
            json.dump(json.loads(service_account_json), temp)
            temp_path = temp.name
        
        # èªè¨¼
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        credentials = Credentials.from_service_account_file(temp_path, scopes=scope)
        gc = gspread.authorize(credentials)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.unlink(temp_path)
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
        try:
            spreadsheet = gc.open_by_key(spreadsheet_id)
        except Exception as e:
            st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
            return False
        
        # ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            worksheet.clear()  # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(filtered_data)
        
        # ãƒ˜ãƒƒãƒ€ã¨å€¤ã‚’åˆ†é›¢
        header = df.columns.tolist()
        values = df.values.tolist()
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
        worksheet.update([header] + values)
        
        return True
        
    except Exception as e:
        st.error(f"Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    st.title("ğŸ” ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æ¤œç´¢ãƒ»é€£çµ¡å…ˆåé›†ãƒ„ãƒ¼ãƒ«")
    st.markdown("""
    ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€æŒ‡å®šã—ãŸã‚¸ãƒ£ãƒ³ãƒ«ã®ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚„ä¼æ¥­ã‚’æ¤œç´¢ã—ã€
    ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚„SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã®é€£çµ¡å…ˆæƒ…å ±ã‚’è‡ªå‹•çš„ã«åé›†ã—ã¾ã™ã€‚
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ - è¨­å®š
    with st.sidebar:
        st.title("æ¤œç´¢è¨­å®š")
        
        st.header("Google APIè¨­å®š")
        google_api_key = st.text_input(
            "Google API Key",
            type="password",
            value=st.session_state.api_keys.get('google_api_key', '')
        )
        
        google_cx = st.text_input(
            "Google Custom Search Engine ID",
            value=st.session_state.api_keys.get('google_cx', '')
        )
        
        if google_api_key and google_cx:
            st.session_state.api_keys['google_api_key'] = google_api_key
            st.session_state.api_keys['google_cx'] = google_cx
            st.success("Google APIè¨­å®šãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        
        st.header("Google Spreadsheetè¨­å®š")
        service_account_json = st.text_area(
            "ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSON",
            value=st.session_state.api_keys.get('google_service_account', ''),
            height=100
        )
        
        if service_account_json:
            try:
                # JSONã¨ã—ã¦è§£æã§ãã‚‹ã‹ç¢ºèª
                json.loads(service_account_json)
                st.session_state.api_keys['google_service_account'] = service_account_json
                st.success("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®šãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            except json.JSONDecodeError:
                st.error("æœ‰åŠ¹ãªJSONã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    # ã‚¿ãƒ–ã‚’ä½œæˆ
    search_tab, results_tab, export_tab = st.tabs(["æ¤œç´¢", "çµæœ", "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"])
    
    # æ¤œç´¢ã‚¿ãƒ–
    with search_tab:
        st.header("ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æ¤œç´¢")
        
        search_keywords = st.text_input(
            "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šå¯èƒ½ï¼‰",
            placeholder="ä¾‹: ã‚¹ãƒ”ãƒªãƒãƒ¥ã‚¢ãƒ« å­è‚²ã¦"
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            num_results = st.slider(
                "å–å¾—ã™ã‚‹çµæœæ•°",
                min_value=10,
                max_value=50,
                value=20,
                step=10
            )
        
        with col2:
            search_categories = st.multiselect(
                "ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆä»»æ„ï¼‰",
                options=["ã‚¹ãƒ”ãƒªãƒãƒ¥ã‚¢ãƒ«", "å­è‚²ã¦", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "æƒ…å ±å•†æ", "ãƒ“ã‚¸ãƒã‚¹", "å¥åº·", "ç¾å®¹", "ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«"],
                default=[]
            )
        
        # è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        with st.expander("è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³"):
            st.slider(
                "ãƒšãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³é–“éš”ï¼ˆç§’ï¼‰",
                min_value=0.5,
                max_value=5.0,
                value=1.0,
                step=0.5,
                help="å„ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹é–“éš”ã§ã™ã€‚çŸ­ã™ãã‚‹ã¨å¯¾è±¡ã‚µã‚¤ãƒˆã«è² è·ãŒã‹ã‹ã‚Šã¾ã™ã€‚"
            )
            
            st.checkbox(
                "Instagramã¨TikTokã‚’å„ªå…ˆ",
                value=True,
                help="æ¤œç´¢çµæœã‹ã‚‰Instagramã¨TikTokã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’å„ªå…ˆçš„ã«æ¢ã—ã¾ã™ã€‚"
            )
        
        # æ¤œç´¢ãƒœã‚¿ãƒ³
        if st.button("æ¤œç´¢é–‹å§‹", type="primary"):
            if not search_keywords:
                st.error("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            elif not ('google_api_key' in st.session_state.api_keys and 'google_cx' in st.session_state.api_keys):
                st.error("Google API Keyã¨Search Engine IDã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            else:
                # æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
                query = search_keywords
                if search_categories:
                    query += " " + " ".join(search_categories)
                
                with st.spinner(f"ã€Œ{query}ã€ã§æ¤œç´¢ä¸­..."):
                    finder = InfluencerFinder()
                    results = finder.search_google(
                        query,
                        st.session_state.api_keys['google_api_key'],
                        st.session_state.api_keys['google_cx'],
                        num_results
                    )
                    
                    if results:
                        st.success(f"{len(results)}ä»¶ã®æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å„ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã‚’åé›†ã—ã¦ã„ã¾ã™...")
                        processed_results = finder.process_search_results(results, num_results)
                        
                        # çµæœã‚’ä¿å­˜
                        st.session_state.search_results = processed_results
                        st.session_state.filtered_results = processed_results
                        
                        # çµæœã‚¿ãƒ–ã«åˆ‡ã‚Šæ›¿ãˆ
                        results_tab.active = True
                        
                        st.success(f"å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ! {len(processed_results)}ä»¶ã®çµæœã‚’åé›†ã—ã¾ã—ãŸã€‚")
                    else:
                        st.error("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
    
    # çµæœã‚¿ãƒ–
    with results_tab:
        if not st.session_state.search_results:
            st.info("ã¾ã æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢ã‚¿ãƒ–ã§æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            st.header("æ¤œç´¢çµæœ")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                filter_instagram = st.checkbox("Instagramã‚ã‚Š", value=False)
            
            with col2:
                filter_tiktok = st.checkbox("TikTokã‚ã‚Š", value=False)
            
            with col3:
                filter_youtube = st.checkbox("YouTubeã‚ã‚Š", value=False)
            
            with col4:
                filter_email = st.checkbox("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Š", value=False)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            filtered_results = st.session_state.search_results
            
            if filter_instagram:
                filtered_results = [item for item in filtered_results if item.get('instagram_url')]
            
            if filter_tiktok:
                filtered_results = [item for item in filtered_results if item.get('tiktok_url')]
            
            if filter_youtube:
                filtered_results = [item for item in filtered_results if item.get('youtube_url')]
            
            if filter_email:
                filtered_results = [item for item in filtered_results if item.get('email')]
            
            st.session_state.filtered_results = filtered_results
            
            # çµæœè¡¨ç¤º
            st.write(f"ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å¾Œ: {len(filtered_results)} ä»¶ã®çµæœ")
            
            if filtered_results:
                # çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
                table_data = []
                for item in filtered_results:
                    table_data.append({
                        "ä¼æ¥­/å€‹äººå": item.get('company_name', ''),
                        "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ": item.get('website_url', ''),
                        "Instagram": "âœ“" if item.get('instagram_url') else "",
                        "TikTok": "âœ“" if item.get('tiktok_url') else "",
                        "YouTube": "âœ“" if item.get('youtube_url') else "",
                        "X": "âœ“" if item.get('x_url') else "",
                        "ãƒ¡ãƒ¼ãƒ«": "âœ“" if item.get('email') else ""
                    })
                
                st.table(pd.DataFrame(table_data))
                
                # è©³ç´°è¡¨ç¤º
                st.subheader("è©³ç´°æƒ…å ±")
                for i, result in enumerate(filtered_results):
                    with st.expander(f"{i+1}. {result['company_name']}"):
                        st.markdown(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {result['title']}")
                        st.markdown(f"**æ¦‚è¦:** {result['snippet']}")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown(f"**ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ:** [{result['website_url']}]({result['website_url']})")
                            if result.get('instagram_url'):
                                st.markdown(f"**Instagram:** [{result['instagram_url']}]({result['instagram_url']})")
                            if result.get('tiktok_url'):
                                st.markdown(f"**TikTok:** [{result['tiktok_url']}]({result['tiktok_url']})")
                        
                        with col2:
                            if result.get('youtube_url'):
                                st.markdown(f"**YouTube:** [{result['youtube_url']}]({result['youtube_url']})")
                            if result.get('x_url'):
                                st.markdown(f"**X (Twitter):** [{result['x_url']}]({result['x_url']})")
                            if result.get('facebook_url'):
                                st.markdown(f"**Facebook:** [{result['facebook_url']}]({result['facebook_url']})")
                        
                        if result.get('email'):
                            st.markdown(f"**ãƒ¡ãƒ¼ãƒ«:** {result['email']}")
            else:
                st.warning("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¿ãƒ–
    with export_tab:
        st.header("ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        if not st.session_state.filtered_results:
            st.info("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢ã‚¿ãƒ–ã§æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            st.write(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãªçµæœ: {len(st.session_state.filtered_results)} ä»¶")
            
            # CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            st.subheader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            
            csv_filename = st.text_input(
                "CSVãƒ•ã‚¡ã‚¤ãƒ«å",
                value=st.session_state.csv_filename
            )
            
            st.session_state.csv_filename = csv_filename
            
            if st.button("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"):
                csv_href = export_to_csv(st.session_state.filtered_results, csv_filename)
                if csv_href:
                    st.markdown(csv_href, unsafe_allow_html=True)
                    st.success(f"{csv_filename} ã®ç”Ÿæˆã«æˆåŠŸã—ã¾ã—ãŸã€‚ä¸Šã®ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            
            # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            st.subheader("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            
            if 'google_service_account' in st.session_state.api_keys and st.session_state.api_keys['google_service_account']:
                col1, col2 = st.columns(2)
                
                with col1:
                    spreadsheet_id = st.text_input("Spreadsheetã®ID", key="spreadsheet_id")
                
                with col2:
                    sheet_name = st.text_input(
                        "ã‚·ãƒ¼ãƒˆå", 
                        value=f"ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼_{datetime.datetime.now().strftime('%Y%m%d')}", 
                        key="sheet_name"
                    )
                
                if st.button("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›"):
                    if not spreadsheet_id:
                        st.error("Spreadsheet IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    else:
                        with st.spinner("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ä¸­..."):
                            success = export_to_google_spreadsheet(
                                st.session_state.filtered_results,
                                st.session_state.api_keys['google_service_account'],
                                spreadsheet_id,
                                sheet_name
                            )
                            if success:
                                st.success(f"ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œ{sheet_name}ã€ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ã—ã¾ã—ãŸï¼")
            else:
                st.warning("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()